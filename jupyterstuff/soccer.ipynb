{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f70d749-7ec4-47b7-9154-c3e9173df06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "menwomen = ['Men','Women']\n",
    "mwcsv = ['olympicsoccer24men.csv','olympicsoccer24women.csv']\n",
    "\n",
    "mw = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90247294-de0d-4899-91d7-ef8a7678848d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'read_csvteamcsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Step 1: Read the CSV file without headers\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csvteamcsv(mwcsv[mw], header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Step 2: Assign column names\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Adjust based on the actual number of columns in the CSV\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'read_csvteamcsv'"
     ]
    }
   ],
   "source": [
    "# use pdf from olympics to get names and teams\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "if True:\n",
    "    # Step 1: Read the CSV file without headers\n",
    "    df = pd.read_csvteamcsv(mwcsv[mw], header=None)\n",
    "    \n",
    "    # Step 2: Assign column names\n",
    "    # Adjust based on the actual number of columns in the CSV\n",
    "    df.columns = ['Team', 'Name']\n",
    "    \n",
    "    # Step 3: Drop rows with any missing or empty values in 'Team' or 'Name'\n",
    "    df = df.dropna(subset=['Team', 'Name'])  # Remove rows with NaN values in 'Team' or 'Name'\n",
    "    df = df[(df['Team'].str.strip() != '') & (df['Name'].str.strip() != '')]  # Remove rows with empty strings\n",
    "    \n",
    "    # Pickle the DataFrame\n",
    "    pickle.dump(df, open('pkls/soccerdf.pkl', 'wb'))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # Load the pickled DataFrame\n",
    "    df = pickle.load(open('pkls/soccerdf.pkl', 'rb'))\n",
    "    \n",
    "# Display the cleaned DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd7ece3-bc65-4187-9c10-47ba6f0f8655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get wiki url for players\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "if True:\n",
    "\n",
    "    # Step 4: Function to get Wikipedia page URL\n",
    "    def get_wikipedia_page_url(name):\n",
    "        search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        params = {\n",
    "            'action': 'query',\n",
    "            'format': 'json',\n",
    "            'list': 'search',\n",
    "            'srsearch': name,\n",
    "            'utf8': 1,\n",
    "            'srlimit': 1\n",
    "        }\n",
    "        response = requests.get(search_url, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'query' in data and 'search' in data['query']:\n",
    "            search_results = data['query']['search']\n",
    "            if search_results:\n",
    "                page_title = search_results[0]['title']\n",
    "                page_url = f\"https://en.wikipedia.org/wiki/{page_title.replace(' ', '_')}\"\n",
    "                return page_url\n",
    "        return None\n",
    "    \n",
    "    # Step 5: Add Wikipedia URLs to DataFrame\n",
    "    df['Name_Link'] = df['Name'].apply(lambda name: get_wikipedia_page_url(name) or None)\n",
    "    \n",
    "    # Add delay to avoid rate limiting\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Pickle the DataFrame\n",
    "    pickle.dump(df, open('pkls/soccerdf.pkl', 'wb'))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # Load the pickled DataFrame\n",
    "    df = pickle.load(open('pkls/soccerdf.pkl', 'rb'))\n",
    "\n",
    "# Display the DataFrame with Wikipedia URLs\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c4a13-d31a-4d30-b89a-312c6579da84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Name_Link'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5a1469-e500-4da7-a260-9f264e6ce90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if webpage is good if not make url=None\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Function to validate URLs\n",
    "def validate_url(row):\n",
    "    name = row['Name']\n",
    "    url = row['Name_Link']\n",
    "    \n",
    "    if pd.isna(url):\n",
    "        return None\n",
    "    \n",
    "    # Extract the part after the last '/'\n",
    "    url_name_part = url.rsplit('/', 1)[-1]\n",
    "    \n",
    "    # Compare the first 3 letters\n",
    "    if url_name_part[:3].lower() == name[:3].lower():\n",
    "        return url\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the validation function to each row\n",
    "df['Name_Link'] = df.apply(validate_url, axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1553b3b-e152-47ee-8210-b88d6485f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Name_Link'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b48edd-4d4e-4736-9567-f9d436e662bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure web pages are ok\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Function to validate URLs\n",
    "def validate_url(row):\n",
    "    name = row['Name']\n",
    "    url = row['Name_Link']\n",
    "    \n",
    "    if pd.isna(url):\n",
    "        return None\n",
    "    \n",
    "    # Extract the part after the last '/'\n",
    "    url_name_part = url.rsplit('/', 1)[-1]\n",
    "    \n",
    "    # Compare the first 3 letters\n",
    "    if url_name_part[:3].lower() == name[:3].lower():\n",
    "        return url\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the validation function to each row\n",
    "df['Name_Link'] = df.apply(validate_url, axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac29bd3-be72-4def-883c-7b69cfea48fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_olympicsoccer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f11dff-8d24-4eb8-9164-7f4930cdb41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in capitals\n",
    "\n",
    "df_capitals = pd.read_csv('capitals.csv', encoding='latin1')\n",
    "df_capitals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8024091d-5056-4ce1-8048-377f469cdd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "if True:\n",
    "\n",
    "    # Function to extract details from Wikipedia infobox\n",
    "    def extract_infobox_details(url):\n",
    "        if url is None:\n",
    "            return None, None, None\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            infobox = soup.find('table', {'class': 'infobox'})\n",
    "            \n",
    "            if infobox is None:\n",
    "                return None, None, None\n",
    "            \n",
    "            # Extracting photo link\n",
    "            img_tag = infobox.find('img')\n",
    "            photo_link = 'https:' + img_tag['src'] if img_tag else None\n",
    "            \n",
    "            # Extracting birthplace and birthplace link\n",
    "            birthplace_data = infobox.find('th', string='Place of birth')\n",
    "            if birthplace_data is None:\n",
    "                return None, None, photo_link  # No birthplace data, return photo link\n",
    "            \n",
    "            birthplace_data = birthplace_data.find_next_sibling('td')\n",
    "            birthplace = birthplace_data.get_text(\" \", strip=True)\n",
    "            birthplace_links = birthplace_data.find_all('a')\n",
    "            birthplace_link = ['https://en.wikipedia.org' + a['href'] for a in birthplace_links]\n",
    "            \n",
    "            return birthplace, birthplace_link, photo_link\n",
    "        except Exception as e:\n",
    "            return None, None, f'Error: {e}'\n",
    "    \n",
    "    # Apply the function to each URL in the DataFrame\n",
    "    def process_row(row):\n",
    "        birthplace, birthplace_link, photo_link = extract_infobox_details(row['Name_Link'])\n",
    "        # Handle None for birthplace and links\n",
    "        if birthplace is None:\n",
    "            birthplace = None\n",
    "        if birthplace_link is None:\n",
    "            birthplace_link = None\n",
    "        return pd.Series([birthplace, birthplace_link, photo_link], index=['birthplace', 'birthplace_link', 'photo_link'])\n",
    "    \n",
    "    # Apply function and add new columns to the DataFrame\n",
    "    df[['birthplace', 'birthplace_link', 'photo_Link']] = df.apply(process_row, axis=1)\n",
    "\n",
    "    # Pickle the DataFrame\n",
    "    pickle.dump(df, open('pkls/soccerdf.pkl', 'wb'))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # Load the pickled DataFrame\n",
    "    df = pickle.load(open('pkls/soccerdf.pkl', 'rb'))\n",
    "\n",
    "# Set pandas options to display full content\n",
    "pd.set_option('display.max_colwidth', None)  # Ensure full content is displayed\n",
    "\n",
    "# Print the DataFrame to see all details\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1d5b52-ac9f-4f25-9983-b89b41e008c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef03b4-665c-4f12-901f-1233a6768b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get summary\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "if True:\n",
    "\n",
    "  # Function to fetch Wikipedia page summary\n",
    "  def fetch_wikipedia_summary(link):\n",
    "      time.sleep(0.2)  # Add a delay of 0.2 seconds between requests\n",
    "      try:\n",
    "          # Extract page title from the link\n",
    "          title = link.split('/')[-1]\n",
    "          url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{title}\"\n",
    "          response = requests.get(url)\n",
    "          data = response.json()\n",
    "          if 'extract' in data:\n",
    "              return data['extract']\n",
    "      except Exception as e:\n",
    "          print(f\"Error fetching summary for {link}: {e}\")\n",
    "      return None\n",
    "\n",
    "  # Apply the function to each row in the DataFrame\n",
    "  df['summary'] = df['Name_Link'].apply(fetch_wikipedia_summary)\n",
    "\n",
    "\n",
    "  # Pickle the DataFrame\n",
    "  pickle.dump(df, open('pkls/soccerdf.pkl', 'wb'))\n",
    "\n",
    "else:\n",
    "\n",
    "  # Load the pickled DataFrame\n",
    "  df = pickle.load(open('pkls/soccerdf.pkl', 'rb'))\n",
    "\n",
    "print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a950ed3b-e5ba-42df-a1aa-dec94293f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "if True: \n",
    "    # Function to fetch coordinates from Wikipedia API with error handling\n",
    "    def fetch_coordinates_from_wikipedia(link):\n",
    "        try:\n",
    "            # Extract page title from the link\n",
    "            title = link.split('/')[-1]\n",
    "            url = f\"https://en.wikipedia.org/w/api.php?action=query&prop=coordinates&format=json&titles={title}\"\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "            pages = data.get('query', {}).get('pages', {})\n",
    "            if pages:\n",
    "                page = next(iter(pages.values()))  # Get the first page (should be only one)\n",
    "                coords_list = page.get('coordinates', [])\n",
    "                if coords_list:\n",
    "                    # Get the first item in the coordinates list\n",
    "                    coords = coords_list[0]\n",
    "                    return (coords.get('lat'), coords.get('lon'))\n",
    "            # If there are no pages or coordinates, return None\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            # Uncomment for debugging\n",
    "            # print(f\"Error fetching coordinates for {link}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Assuming df['birthplace_link'] contains lists of URLs\n",
    "    def get_first_coords_from_list(row):\n",
    "        if isinstance(row, list) and row:\n",
    "            first_url = row[0]\n",
    "            return fetch_coordinates_from_wikipedia(first_url)\n",
    "        return None\n",
    "    \n",
    "    # Example DataFrame creation\n",
    "    # df = pd.DataFrame({'birthplace_link': [['https://en.wikipedia.org/wiki/Some_City'], ['https://en.wikipedia.org/wiki/Another_City']]})\n",
    "    \n",
    "    # Apply the function to get coordinates for the first URL in the list\n",
    "    df['birth_coords'] = df['birthplace_link'].apply(get_first_coords_from_list)\n",
    "\n",
    "    # Pickle the DataFrame\n",
    "    pickle.dump(df, open('pkls/soccerdf.pkl', 'wb'))\n",
    "\n",
    "else:\n",
    "\n",
    "    # Load the pickled DataFrame\n",
    "    df = pickle.load(open('pkls/soccerdf.pkl', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d93916-6565-439d-9493-f1801af47008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make capital in home not known\n",
    "\n",
    "# Remove duplicate countries in df_capitals\n",
    "df_capitals_unique = df_capitals.drop_duplicates(subset='Country')\n",
    "\n",
    "# Create a mapping from Country to Coordinates\n",
    "team_to_coords = df_capitals_unique.set_index('Country')[['Latitude', 'Longitude']].to_dict(orient='index')\n",
    "\n",
    "# Function to fill missing coordinates\n",
    "def fill_missing_coords(row):\n",
    "    if row['birth_coords'] is None:\n",
    "        team = row['Team']\n",
    "        coords = team_to_coords.get(team)\n",
    "        if coords:\n",
    "            return (coords['Latitude'], coords['Longitude'])\n",
    "    return row['birth_coords']\n",
    "\n",
    "# Apply the function to fill missing coordinates\n",
    "df['birth_coords'] = df.apply(fill_missing_coords, axis=1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9537853-54cd-4f90-93ca-6f2457ce0e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save with correct gender\n",
    "\n",
    "    # Pickle the DataFrame\n",
    "    pickle.dump(df, open('pkls/soccer'+menwomen[mw]+'df.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
