{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b89be7d-105c-42c2-8a81-58b1de3cd82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "todo:\n",
    "- home button??\n",
    "- retrieve stuff\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684e7c0-f414-4867-9697-172c92515333",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "#read in dinosauria from pbdb\n",
    "fname= '/content/drive/MyDrive/paleo/df_all.pkl'\n",
    "fromfile=True\n",
    "df_all = {}\n",
    "clades = [\"dinosauria\",\"trilobita\",\"mammalia\", \"pterosauria\", \"ichthyosauria\", \"plesiosauria\",\\\n",
    "          \"mosasaurus\",\"synapsida\", \"crocodylomorpha\", \"ammonites\"]\n",
    "\n",
    "if fromfile:\n",
    "  # Read the pickle file into a DataFrame\n",
    "  with open(fname, 'rb') as file:\n",
    "    df_all = pickle.load(file)\n",
    "else:\n",
    "  # Load from pbdb info and coordinates\n",
    "  for clade in clades:\n",
    "    # Define the URL for the CSV file -- note older that 64 million\n",
    "    url = 'https://paleobiodb.org/data1.2/occs/list.csv?base_name='+\\\n",
    "      clade + '&min_ma=1&show=class,coords'\n",
    "    # Read the  data directly into a pandas DataFrame\n",
    "    #print(clade)\n",
    "    df_all[clade] = pd.read_csv(url)\n",
    "    #save to file for later\n",
    "  with open(fname, 'wb') as file:\n",
    "    pickle.dump(df_all, file)\n",
    "\n",
    "\n",
    "for clade in clades:\n",
    "  print(clade,': ',df_all[clade].shape)\n",
    "for clade in clades:\n",
    "  print(df_all[clade].head(3))\n",
    "\n",
    "\n",
    "# Round the latitude and longitude columns to 2 decimal places\n",
    "df_rounded={}\n",
    "for clade in clades:\n",
    "  df_rounded[clade] = df_all[clade].round({'lat': 2, 'lng': 2})\n",
    "\n",
    "print(df_rounded[clades[0]].columns)\n",
    "\n",
    "\n",
    "\n",
    "# just keep relevant columns\n",
    "df_thin= {}\n",
    "for clade in clades:\n",
    "  df_thin[clade] = df_rounded[clade][['lat', 'lng', 'accepted_name','max_ma', 'collection_no']]\n",
    "\n",
    "print(df_thin[clades[0]].columns)\n",
    "\n",
    "#aggregate by location, so no duplicates\n",
    "df_aggregated = {}\n",
    "for clade in clades:\n",
    "  df= df_thin[clade]\n",
    "  df_aggregated[clade] = df.groupby(['lat', 'lng']).agg({\n",
    "  'accepted_name': lambda x: list(set(x)),\n",
    "  'max_ma': lambda x: list(set(x)),\n",
    "  'collection_no': lambda x: list(set(x))\n",
    "  }).reset_index()\n",
    "\n",
    "for clade in clades:\n",
    "  print(clade, df_aggregated[clade].shape)\n",
    "print(df_aggregated[clades[0]].head(3))\n",
    "print(df_aggregated[clades[0]].columns)\n",
    "\n",
    "\n",
    "for clade in clades[:1]:\n",
    "  print(clade,len(max(df_aggregated[clade]['accepted_name'], key=len)))\n",
    "  print(clade,len(max(df_aggregated[clade]['max_ma'], key=len)))\n",
    "  print(clade,len(max(df_aggregated[clade]['collection_no'], key=len)))\n",
    "\n",
    "\n",
    "# split rows with too many names\n",
    "def split_rows(df):\n",
    "    new_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        names = row['accepted_name']\n",
    "        if len(names) > 5:\n",
    "            for i in range(5, len(names), 5):\n",
    "                new_row = row.copy()\n",
    "                new_row['accepted_name'] = names[i:i + 5]\n",
    "                new_rows.append(new_row)\n",
    "        else:\n",
    "            new_rows.append(row)\n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "fname= '/content/drive/MyDrive/paleo/df_split.pkl'\n",
    "fromfile=True\n",
    "\n",
    "if fromfile:\n",
    "  # Read the pickle file into a DataFrame\n",
    "  with open(fname, 'rb') as file:\n",
    "    df_split = pickle.load(file)\n",
    "else:\n",
    "  # Convert the aggregated DataFrame to a split DataFrame\n",
    "  df_split = {}\n",
    "  for clade in df_aggregated:\n",
    "    df_split[clade] = split_rows(df_aggregated[clade])\n",
    "    #save to file for later\n",
    "  with open(fname, 'wb') as file:\n",
    "    pickle.dump(df_split, file)\n",
    "\n",
    "\n",
    "\n",
    "# Make name, family, order, class\n",
    "# *** we will use order since it seems to exist and in Wikipedia usually\n",
    "df_names = {}\n",
    "for clade in clades:\n",
    "    df_names[clade] = df_all[clade].groupby('accepted_name').agg({\n",
    "        'order': 'first'  # Take the first occurrence\n",
    "    }).reset_index()\n",
    "\n",
    "print(df_names[clades[1]].shape)\n",
    "print(df_names[clades[1]].head(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfc02f5-714f-4fc5-8828-8a4dd28f5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Google Tag Manager and AdSense scripts to the map\n",
    "gtag_script = \"\"\"\n",
    "<!-- Google tag (gtag.js) -->\n",
    "<script async src=\"https://www.googletagmanager.com/gtag/js?id=G-2RB0G9NLE8\"></script>\n",
    "<script>\n",
    "  window.dataLayer = window.dataLayer || [];\n",
    "  function gtag(){dataLayer.push(arguments);}\n",
    "  gtag('js', new Date());\n",
    "\n",
    "  gtag('config', 'G-2RB0G9NLE8');\n",
    "</script>\n",
    "\n",
    "<script async src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4771147286272001\"\n",
    "     crossorigin=\"anonymous\"></script>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b436901-1937-4fd6-9a30-47bc4ec88a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**** use wiki to grab stuff in advance so it doesn't need so much live javascript!\n",
    "\n",
    "# Works great, except doesn't say not found!\n",
    "\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "from folium.plugins import LocateControl, MarkerCluster\n",
    "from branca.element import Element\n",
    "\n",
    "def mk_map(df_split, tclades, mindate, maxdate, df_names):\n",
    "    # Convert the aggregated DataFrame to a GeoDataFrame\n",
    "    geometry, geo_df, sgdf = {}, {}, {}\n",
    "    for clade in tclades:\n",
    "        geometry[clade] = [Point(xy) for xy in zip(df_split[clade]['lng'], df_split[clade]['lat'])]\n",
    "        geo_df[clade] = gpd.GeoDataFrame(df_split[clade], geometry=geometry[clade])\n",
    "        sgdf[clade] = geo_df[clade]\n",
    "\n",
    "    # Create a Folium map centered on California with scroll wheel zoom enabled\n",
    "    m = folium.Map(location=[36.7783, -119.4179], zoom_start=6, control_scale=True, scrollWheelZoom=True)\n",
    "\n",
    "    # Add LocateControl to the map\n",
    "    LocateControl().add_to(m)\n",
    "\n",
    "    # Define a set of colors for the clades\n",
    "    colors = ['blue', 'green', 'red', 'purple', 'orange', 'darkred', 'lightred', 'beige', 'darkblue', 'lightblue']\n",
    "\n",
    "    # JavaScript function to fetch Wikipedia data including the image\n",
    "    js_function = \"\"\"\n",
    "        function fetchWikipediaData(name, order, elementId) {\n",
    "        let simpleWikiUrl = 'https://simple.wikipedia.org/api/rest_v1/page/summary/' + name;\n",
    "        let enWikiUrl = 'https://en.wikipedia.org/api/rest_v1/page/summary/' + name;\n",
    "\n",
    "        fetch(simpleWikiUrl)\n",
    "            .then(response => {\n",
    "                if (response.ok) {\n",
    "                    return response.json();\n",
    "                } else {\n",
    "                    return fetch(enWikiUrl);\n",
    "                }\n",
    "            })\n",
    "            .then(response => {\n",
    "                if (response.ok) {\n",
    "                    return response.json();\n",
    "                } else {\n",
    "                    // If the name fetch failed, try fetching the order\n",
    "                    let orderUrl = 'https://en.wikipedia.org/api/rest_v1/page/summary/' + order;\n",
    "                    return fetch(orderUrl).then(response => {\n",
    "                        if (response.ok) {\n",
    "                            return response.json();\n",
    "                        } else {\n",
    "                            throw new Error('No information found');\n",
    "                        }\n",
    "                    });\n",
    "                }\n",
    "            })\n",
    "            .then(data => {\n",
    "                let content = '';\n",
    "                if (data.thumbnail && data.thumbnail.source) {\n",
    "                    content += '<img src=\"' + data.thumbnail.source + '\" width=\"150\"><br>';\n",
    "                }\n",
    "                if (data.title.toLowerCase() === name.toLowerCase()) {\n",
    "                    content += `<b>${name}</b><br>`;\n",
    "                    content += data.extract + '<br>';\n",
    "                    content += '<a href=\"https://en.wikipedia.org/wiki/' + name + '\" target=\"_blank\">Read more on Wikipedia</a>';\n",
    "                } else {\n",
    "                    content += `'${name}' not found in wikipedia. Showing the order.<br>`;\n",
    "                    content += `<b>${data.title}</b> (Order Information)<br>`;\n",
    "                    content += data.extract + '<br>';\n",
    "                    content += '<a href=\"https://en.wikipedia.org/wiki/' + order + '\" target=\"_blank\">Read more about the order on Wikipedia</a>';\n",
    "                }\n",
    "                document.getElementById(elementId).innerHTML = content;\n",
    "            })\n",
    "            .catch(error => {\n",
    "                console.error('Error fetching data from Wikipedia API:', error);\n",
    "                document.getElementById(elementId).innerHTML = `Error: No information found for '${name}' or its order '${order}'`;\n",
    "            });\n",
    "\n",
    "        // Highlight the clicked name\n",
    "        var nameElement = document.getElementById('name-' + name.replace(/ /g, '_'));\n",
    "        if (nameElement) {\n",
    "            nameElement.style.fontWeight = 'bold';\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    # Add JavaScript function to the map\n",
    "    m.get_root().html.add_child(folium.Element(f'<script>{js_function}</script>'))\n",
    "\n",
    "    # Add points to the MarkerCluster without layers\n",
    "    marker_cluster = MarkerCluster().add_to(m)\n",
    "    for i in range(len(tclades)):\n",
    "        clade = tclades[i]\n",
    "        color = colors[i % len(colors)]  # Add color to the marker\n",
    "        for idx, row in sgdf[clade].iterrows():\n",
    "            if row['max_ma'][0] >= mindate and row['max_ma'][0] <= maxdate:\n",
    "                # Prepare the HTML content for the popup\n",
    "                max_ma_list = ', '.join(map(str, row['max_ma']))\n",
    "                collection_no_links = ''.join([f'<a href=\"https://paleobiodb.org/classic/basicCollectionSearch?collection_no={no}\" target=\"_blank\">collection</a><br>' for no in row['collection_no']])\n",
    "\n",
    "                names_links = ''.join([\n",
    "                    f'<a id=\"name-{str(name).replace(\" \", \"_\")}\" href=\"#\" onclick=\"fetchWikipediaData(\\'{str(name).replace(\" \", \"_\")}\\', \\'{str(df_names[clade].loc[df_names[clade][\"accepted_name\"] == name, \"order\"].values[0]).replace(\" \", \"_\")}\\', \\'wiki-content-{idx}\\'); return false;\" style=\"font-size: 16px;\">{name}</a><br>'\n",
    "                    if pd.notna(name) else\n",
    "                    f'<b>Name not found, but it is a member of the order \\'{str(df_names[clade].loc[df_names[clade][\"accepted_name\"] == name, \"order\"].values[0])}\\'</b><br>'\n",
    "                    for name in row['accepted_name']\n",
    "                ])\n",
    "                element_id = f'wiki-content-{idx}'\n",
    "                html = f'''\n",
    "                    <div style=\"text-align: center;\">\n",
    "                        <div style=\"font-size: larger;\">\n",
    "                            <b>About</b> {max_ma_list} <b>Million Years Old</b>\n",
    "                        </div>\n",
    "                        <br>\n",
    "                        <div style=\"font-size: smaller;\">\n",
    "                            {collection_no_links}\n",
    "                        </div>\n",
    "                        <div id=\"name-container-{idx}\">\n",
    "                            {names_links}\n",
    "                        </div>\n",
    "                        <div id=\"{element_id}\" style=\"height: 200px; overflow-y: auto; margin-top: 10px;\"></div>\n",
    "                    </div>\n",
    "                '''\n",
    "                popup = folium.Popup(html, max_width='85%', min_width='60%')\n",
    "                folium.Marker(\n",
    "                    location=[row['lat'], row['lng']],\n",
    "                    popup=popup,\n",
    "                    icon=folium.Icon(color=color),  # Add color to the marker\n",
    "                ).add_to(marker_cluster)\n",
    "\n",
    "    # Add the Google Analytics tracking code to the map\n",
    "    element = Element(analytics)\n",
    "    m.get_root().html.add_child(element)\n",
    "\n",
    "    # Save the map to an HTML file\n",
    "    cladenames = '_'.join(tclades + [str(mindate), str(maxdate)])\n",
    "    fnamemap = '/content/drive/MyDrive/paleomapstuff/fossil_map' + cladenames + '.html'\n",
    "    m.save(fnamemap)\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a8c519-946a-47e7-af60-6e709b615c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# make maps - one at a time\n",
    "\n",
    "#clades = [\"dinosauria\",\"trilobita\",\"mammalia\", \"pterosauria\", \"ichthyosauria\", \"plesiosauria\",\\\n",
    "#         \"mosasaurus\",\"synapsida\", \"crocodylomorpha\", \"ammonites\"]\n",
    "\n",
    "# control dates allowed\n",
    "if True:  # for testing \"trilobita\" small\n",
    "    mindate, maxdate = 440, 445  # for everything except mammals\n",
    "    tclades = clades[1:2]\n",
    "elif False:  # dinosauria \"pterosauria\", \"ichthyosauria\", \"plesiosauria\",\"mosasaurus\", \"crocodylomorpha\"\n",
    "    mindate, maxdate = 66, 1000  # for everything except mammals\n",
    "    tclades = clades[0:1]  + clades[3:7]+ clades[8:9]\n",
    "elif False:  # trilobita\n",
    "    mindate, maxdate = 66, 1000  # for everything except mammals\n",
    "    tclades = clades[1:2]\n",
    "elif False:  # 'old mammalia'\n",
    "    mindate, maxdate = 30, 1000  # for mammals\n",
    "    tclades = clades[2:3]\n",
    "else:  # 'young mammalia'\n",
    "    mindate, maxdate = 1, 300  # for mammals\n",
    "    tclades = clades[2:3]\n",
    "\n",
    "m=mk_map(df_split, tclades, mindate, maxdate, df_names)\n",
    "\n",
    "\n",
    "\n",
    "# Display the map\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee4b3e9-2b47-416a-acbd-f01057dc9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# make all\n",
    "\n",
    "#clades = [\"dinosauria\",\"trilobita\",\"mammalia\", \"pterosauria\", \"ichthyosauria\", \"plesiosauria\",\\\n",
    "#         \"mosasaurus\",\"synapsida\", \"crocodylomorpha\", \"ammonites\"]\n",
    "\n",
    "# dinosauria \"pterosauria\", \"ichthyosauria\", \"plesiosauria\",\"mosasaurus\", \"crocodylomorpha\"\n",
    "mindate, maxdate = 66, 1000  # for everything except mammals\n",
    "tclades = clades[0:1]  + clades[3:7]+ clades[8:9]\n",
    "m=mk_map(df_split, tclades, mindate, maxdate, df_names)\n",
    "\n",
    "# trilobita\n",
    "mindate, maxdate = 66, 1000  # for everything except mammals\n",
    "tclades = clades[1:2]\n",
    "m=mk_map(df_split, tclades, mindate, maxdate, df_names)\n",
    "\n",
    "# 'old mammalia'\n",
    "mindate, maxdate = 30, 1000  # for mammals\n",
    "tclades = clades[2:3]\n",
    "m=mk_map(df_split, tclades, mindate, maxdate, df_names)\n",
    "\n",
    "# 'young mammalia'\n",
    "mindate, maxdate = 1, 30  # for mammals\n",
    "tclades = clades[2:3]\n",
    "m=mk_map(df_split, tclades, mindate, maxdate, df_names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the map\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
